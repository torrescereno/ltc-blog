---
title: "PySpark SQL: Guía Práctica para Procesamiento de Datos"
description: "Aprende a procesar grandes volúmenes de datos con PySpark SQL. Desde la creación de sesiones hasta operaciones avanzadas como pivot, cache y optimización de performance."
date: "2024-08-16"
category: "Desarrollo"
tags: ["python", "pyspark", "big-data", "spark", "data-engineering"]
---

PySpark SQL es un módulo poderoso para trabajar con datos estructurados y semi-estructurados a gran escala. Esta guía cubre los conceptos fundamentales y técnicas avanzadas.

<Callout type="info">
  Para estos ejemplos se usó el [dataset de
  JJ.OO](https://github.com/torrescereno/spark-notes/tree/main/data). Descárgalo
  para seguir la guía paso a paso.
</Callout>

## 1. Session

```python
from pyspark.sql import SparkSession

sp = SparkSession.builder.appName("app").getOrCreate()
sp.sparkContext.setLogLevel("ERROR")
```

## 2. Lectura de Datos y Schema

Definir el schema explícitamente mejora el rendimiento:

```python
from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType

schema = StructType([
    StructField("date", TimestampType(), True),
    StructField("stage_code", StringType(), True),
    StructField("event_code", StringType(), True),
    StructField("event_name", StringType(), True),
    StructField("participant_name", StringType(), True),
    StructField("participant_country", StringType(), True),
    StructField("rank", IntegerType(), True),
    StructField("result", StringType(), True),
])

df = sp.read \
    .format("csv") \
    .option("header", True) \
    .schema(schema) \
    .load("data/results")
```

<Callout type="tip">
  **Tip de rendimiento:** Definir el schema explícitamente evita que Spark tenga
  que leer todo el archivo para inferir los tipos, lo cual puede ser muy costoso
  en datasets grandes.
</Callout>

Alternativamente, puedes inferir el schema:

```python
df = sp.read.option("header", True) \
    .option("inferSchema", True) \
    .csv("data/results")
```

## 3. Consultas SQL

Crea una vista temporal para usar SQL:

```python
df.createOrReplaceTempView("jjoo")

result = sp.sql("""
    SELECT *
    FROM jjoo
    WHERE participant_country = 'Chile'
""")
result.show(5)
```

## 4. Operaciones sobre Columnas

### Filters

```python
from pyspark.sql import functions as F

# Usando filter
df.filter(df["participant_country"] == "Chile") \
    .select("participant_name") \
    .distinct() \
    .show()

# Usando where (más expresivo)
df.where(
    (F.col("participant_country") == "Chile") &
    (F.col("gender").isin(["M", "W"]))
).count()
```

### Transformaciones

```python
# Agregar columna
df.withColumn("upper_name", F.upper(df.participant_name))

# Renombrar columna
df.withColumnRenamed("date", "fecha")
```

## 5. Agrupaciones y Agregaciones

```python
# Eventos por disciplina
df.groupBy("discipline_name") \
    .agg(F.countDistinct("event_name").alias("unique_events")) \
    .show(5)

# Países con más participantes
df.groupBy("participant_country") \
    .agg(F.countDistinct("participant_name").alias("athletes")) \
    .orderBy(F.desc("athletes")) \
    .show(5)
```

## 6. Pivot

Transforma filas en columnas:

```python
# Participantes por género en cada país
df.groupBy("participant_country") \
    .pivot("gender") \
    .agg(F.count("participant_code")) \
    .orderBy(F.asc("participant_country")) \
    .show(5)
```

## 7. Optimización de Performance

### Cache

Almacena DataFrames en memoria para reutilización:

```python
from pyspark import StorageLevel

# Cache básico (memoria y disco)
df.cache()

# Control granular
from pyspark import StorageLevel
df.persist(StorageLevel.MEMORY_ONLY)
df.persist(StorageLevel.DISK_ONLY)
df.persist(StorageLevel.MEMORY_AND_DISK)

# Liberar memoria
df.unpersist()
```

### Plan de Ejecución

Analiza cómo Spark ejecutará tus operaciones:

```python
# Plan lógico y físico
df.groupBy("participant_code").count().explain(mode="cost")

# Solo plan físico
df.explain()
```

### Repartition

Optimiza la distribución de datos:

```python
# Redistribuir a 20 particiones por columna
df_repartitioned = df.repartition(20, "participant_code")

# Aplicar función a cada partición
def process_partition(rows):
    count = sum(1 for _ in rows)
    print(f"Partition has {count} records")

df_repartitioned.foreachPartition(process_partition)
```

## Mejores Prácticas

<Callout type="warning">
  **Atención:** Los DataFrames son evaluados perezosamente (lazy evaluation).
  Las operaciones no se ejecutan hasta que llamas una acción como `.show()`,
  `.collect()` o `.write`.
</Callout>

1. **Define schemas explícitamente** para evitar inferencia costosa
2. **Usa cache** cuando reutilizarás un DataFrame múltiples veces
3. **Filtra temprano** para reducir el volumen de datos procesados
4. **Revisa el plan de ejecución** para identificar cuellos de botella
5. **Ajusta particiones** según el tamaño de tu clúster

## Referencias

- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/reference/index.html)
- [cartershanklin/pyspark-cheatsheet](https://github.com/cartershanklin/pyspark-cheatsheet)
- [kevinschaich/pyspark-cheatsheet](https://github.com/kevinschaich/pyspark-cheatsheet)
